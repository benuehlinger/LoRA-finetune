{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install peft==0.4.0 datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-30T15:13:28.542475Z",
          "iopub.execute_input": "2024-07-30T15:13:28.542840Z",
          "iopub.status.idle": "2024-07-30T15:13:42.436803Z",
          "shell.execute_reply.started": "2024-07-30T15:13:28.542810Z",
          "shell.execute_reply": "2024-07-30T15:13:42.435793Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "rFMeFiHEuZpM",
        "outputId": "fcdd3767-bf1d-410f-a6d1-7cb54d1f7e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.42.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.19.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a look at the possible modules\n",
        ""
      ],
      "metadata": {
        "id": "33JinEqluZpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "def print_modules(model, indent=0):\n",
        "    for name, module in model.named_children():\n",
        "        print('  ' * indent + f\" {name}: {type(module).__name__}\")\n",
        "        if list(module.children()):\n",
        "            print_modules(module, indent + 1)\n",
        "        else:\n",
        "            for param_name, param in module.named_parameters(recurse=False):\n",
        "                print('  ' * (indent + 1) + f\" {param_name}: {param.shape}\")\n",
        "\n",
        "# Load the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Print all modules\n",
        "print(\"All modules in DistilBERT:\")\n",
        "print_modules(model)\n",
        "\n",
        "# Print all module names in a flat list\n",
        "print(\"\\nAll module names (flat list):\")\n",
        "for name, _ in model.named_modules():\n",
        "    print(name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-30T15:54:48.823217Z",
          "iopub.execute_input": "2024-07-30T15:54:48.824071Z",
          "iopub.status.idle": "2024-07-30T15:54:49.283919Z",
          "shell.execute_reply.started": "2024-07-30T15:54:48.824029Z",
          "shell.execute_reply": "2024-07-30T15:54:49.282941Z"
        },
        "trusted": true,
        "id": "Lo6vXkrCuZpQ",
        "outputId": "62528d0c-8ce8-41bc-a108-89b3b70ecefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "All modules in DistilBERT:\n distilbert: DistilBertModel\n   embeddings: Embeddings\n     word_embeddings: Embedding\n       weight: torch.Size([30522, 768])\n     position_embeddings: Embedding\n       weight: torch.Size([512, 768])\n     LayerNorm: LayerNorm\n       weight: torch.Size([768])\n       bias: torch.Size([768])\n     dropout: Dropout\n   transformer: Transformer\n     layer: ModuleList\n       0: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n       1: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n       2: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n       3: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n       4: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n       5: TransformerBlock\n         attention: MultiHeadSelfAttention\n           dropout: Dropout\n           q_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           k_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           v_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n           out_lin: Linear\n             weight: torch.Size([768, 768])\n             bias: torch.Size([768])\n         sa_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n         ffn: FFN\n           dropout: Dropout\n           lin1: Linear\n             weight: torch.Size([3072, 768])\n             bias: torch.Size([3072])\n           lin2: Linear\n             weight: torch.Size([768, 3072])\n             bias: torch.Size([768])\n           activation: GELUActivation\n         output_layer_norm: LayerNorm\n           weight: torch.Size([768])\n           bias: torch.Size([768])\n pre_classifier: Linear\n   weight: torch.Size([768, 768])\n   bias: torch.Size([768])\n classifier: Linear\n   weight: torch.Size([2, 768])\n   bias: torch.Size([2])\n dropout: Dropout\n\nAll module names (flat list):\n\ndistilbert\ndistilbert.embeddings\ndistilbert.embeddings.word_embeddings\ndistilbert.embeddings.position_embeddings\ndistilbert.embeddings.LayerNorm\ndistilbert.embeddings.dropout\ndistilbert.transformer\ndistilbert.transformer.layer\ndistilbert.transformer.layer.0\ndistilbert.transformer.layer.0.attention\ndistilbert.transformer.layer.0.attention.dropout\ndistilbert.transformer.layer.0.attention.q_lin\ndistilbert.transformer.layer.0.attention.k_lin\ndistilbert.transformer.layer.0.attention.v_lin\ndistilbert.transformer.layer.0.attention.out_lin\ndistilbert.transformer.layer.0.sa_layer_norm\ndistilbert.transformer.layer.0.ffn\ndistilbert.transformer.layer.0.ffn.dropout\ndistilbert.transformer.layer.0.ffn.lin1\ndistilbert.transformer.layer.0.ffn.lin2\ndistilbert.transformer.layer.0.ffn.activation\ndistilbert.transformer.layer.0.output_layer_norm\ndistilbert.transformer.layer.1\ndistilbert.transformer.layer.1.attention\ndistilbert.transformer.layer.1.attention.dropout\ndistilbert.transformer.layer.1.attention.q_lin\ndistilbert.transformer.layer.1.attention.k_lin\ndistilbert.transformer.layer.1.attention.v_lin\ndistilbert.transformer.layer.1.attention.out_lin\ndistilbert.transformer.layer.1.sa_layer_norm\ndistilbert.transformer.layer.1.ffn\ndistilbert.transformer.layer.1.ffn.dropout\ndistilbert.transformer.layer.1.ffn.lin1\ndistilbert.transformer.layer.1.ffn.lin2\ndistilbert.transformer.layer.1.ffn.activation\ndistilbert.transformer.layer.1.output_layer_norm\ndistilbert.transformer.layer.2\ndistilbert.transformer.layer.2.attention\ndistilbert.transformer.layer.2.attention.dropout\ndistilbert.transformer.layer.2.attention.q_lin\ndistilbert.transformer.layer.2.attention.k_lin\ndistilbert.transformer.layer.2.attention.v_lin\ndistilbert.transformer.layer.2.attention.out_lin\ndistilbert.transformer.layer.2.sa_layer_norm\ndistilbert.transformer.layer.2.ffn\ndistilbert.transformer.layer.2.ffn.dropout\ndistilbert.transformer.layer.2.ffn.lin1\ndistilbert.transformer.layer.2.ffn.lin2\ndistilbert.transformer.layer.2.ffn.activation\ndistilbert.transformer.layer.2.output_layer_norm\ndistilbert.transformer.layer.3\ndistilbert.transformer.layer.3.attention\ndistilbert.transformer.layer.3.attention.dropout\ndistilbert.transformer.layer.3.attention.q_lin\ndistilbert.transformer.layer.3.attention.k_lin\ndistilbert.transformer.layer.3.attention.v_lin\ndistilbert.transformer.layer.3.attention.out_lin\ndistilbert.transformer.layer.3.sa_layer_norm\ndistilbert.transformer.layer.3.ffn\ndistilbert.transformer.layer.3.ffn.dropout\ndistilbert.transformer.layer.3.ffn.lin1\ndistilbert.transformer.layer.3.ffn.lin2\ndistilbert.transformer.layer.3.ffn.activation\ndistilbert.transformer.layer.3.output_layer_norm\ndistilbert.transformer.layer.4\ndistilbert.transformer.layer.4.attention\ndistilbert.transformer.layer.4.attention.dropout\ndistilbert.transformer.layer.4.attention.q_lin\ndistilbert.transformer.layer.4.attention.k_lin\ndistilbert.transformer.layer.4.attention.v_lin\ndistilbert.transformer.layer.4.attention.out_lin\ndistilbert.transformer.layer.4.sa_layer_norm\ndistilbert.transformer.layer.4.ffn\ndistilbert.transformer.layer.4.ffn.dropout\ndistilbert.transformer.layer.4.ffn.lin1\ndistilbert.transformer.layer.4.ffn.lin2\ndistilbert.transformer.layer.4.ffn.activation\ndistilbert.transformer.layer.4.output_layer_norm\ndistilbert.transformer.layer.5\ndistilbert.transformer.layer.5.attention\ndistilbert.transformer.layer.5.attention.dropout\ndistilbert.transformer.layer.5.attention.q_lin\ndistilbert.transformer.layer.5.attention.k_lin\ndistilbert.transformer.layer.5.attention.v_lin\ndistilbert.transformer.layer.5.attention.out_lin\ndistilbert.transformer.layer.5.sa_layer_norm\ndistilbert.transformer.layer.5.ffn\ndistilbert.transformer.layer.5.ffn.dropout\ndistilbert.transformer.layer.5.ffn.lin1\ndistilbert.transformer.layer.5.ffn.lin2\ndistilbert.transformer.layer.5.ffn.activation\ndistilbert.transformer.layer.5.output_layer_norm\npre_classifier\nclassifier\ndropout\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tune the model using LoRA and commit to Hugging Face"
      ],
      "metadata": {
        "id": "MJXV4tNfuheR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for Hugging Face access token\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "\n",
        "# Set up GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset('glue', 'sst2')\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence'], truncation=True, padding=True)\n",
        "\n",
        "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load the accuracy metric\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "# Define the compute_metrics function\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Set up training arguments for GPU\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    push_to_hub=True,\n",
        "    hub_token=hf_token,\n",
        "    fp16=True  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Print model architecture\n",
        "print(model)\n",
        "\n",
        "# Identify potential target modules\n",
        "target_modules = [name for name, module in model.named_modules() if \"lin\" in name]\n",
        "print(\"Potential target modules:\", target_modules)\n",
        "\n",
        "# Define LoRA Config\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=target_modules\n",
        ")\n",
        "\n",
        "# Get the PEFT model and move to GPU\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.to(device)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate after fine-tuning\n",
        "print(\"Evaluating after fine-tuning...\")\n",
        "post_finetune_results = trainer.evaluate()\n",
        "print(\"Results after fine-tuning:\", post_finetune_results)\n",
        "\n",
        "# Save the PEFT model locally\n",
        "peft_model.save_pretrained('./peft-model')\n",
        "\n",
        "# Push the PEFT model to Hugging Face's Model Hub\n",
        "peft_model.push_to_hub(\"my-peft-distilbert\", use_auth_token=hf_token)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-30T15:13:52.003233Z",
          "iopub.execute_input": "2024-07-30T15:13:52.003627Z",
          "iopub.status.idle": "2024-07-30T15:30:45.972057Z",
          "shell.execute_reply.started": "2024-07-30T15:13:52.003594Z",
          "shell.execute_reply": "2024-07-30T15:30:45.969935Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "7e841e34797c47cba7a59cb5eead55ab",
            "fc78134c6fd94c5b90601492a3b0d17f",
            "e8f1ff7e96f840ecbe59dbc61dce12a2",
            "762158df435741bbbbd7a9df39f5fe05",
            "36b5310f7a5a4c38938670530ad88b18",
            "c7be04171f8642a89e5dacbb15c576b7",
            "b116f81214794953a558a4b109947c8d",
            "3ed7456c85d345049a288c750ac0f403",
            "d069e4f178c445f0881587fc7a66e871",
            "a3c3ffab2952435fbadd5c858a4141cb",
            "ac7757d504b841719ac06d991fc81a45",
            "1ac6179967dc4bf5a983fc73c744f7fb",
            "90b53b75cddc44478180cff50ba019c3",
            "7a9ee56c06d541ef84fd489c016251ea",
            "3b15564a4c6c4fc18fb0aba8bdb09b86",
            "d458d46c91ac4701ac874da26d13c8d9",
            "3e99fa74517a49eaaccc24f10e664a9a"
          ]
        },
        "id": "TODZKZVEuZpR",
        "outputId": "81ebdefb-80ba-405a-94b7-eb56ec474395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Enter your Hugging Face access token:  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e841e34797c47cba7a59cb5eead55ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc78134c6fd94c5b90601492a3b0d17f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8f1ff7e96f840ecbe59dbc61dce12a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "762158df435741bbbbd7a9df39f5fe05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36b5310f7a5a4c38938670530ad88b18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7be04171f8642a89e5dacbb15c576b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b116f81214794953a558a4b109947c8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ed7456c85d345049a288c750ac0f403"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d069e4f178c445f0881587fc7a66e871"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3c3ffab2952435fbadd5c858a4141cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac7757d504b841719ac06d991fc81a45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ac6179967dc4bf5a983fc73c744f7fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/872 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90b53b75cddc44478180cff50ba019c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a9ee56c06d541ef84fd489c016251ea"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_34/3367601975.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric('accuracy')\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b15564a4c6c4fc18fb0aba8bdb09b86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "The repository for accuracy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/accuracy.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d458d46c91ac4701ac874da26d13c8d9"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\nPotential target modules: ['distilbert.transformer.layer.0.attention.q_lin', 'distilbert.transformer.layer.0.attention.k_lin', 'distilbert.transformer.layer.0.attention.v_lin', 'distilbert.transformer.layer.0.attention.out_lin', 'distilbert.transformer.layer.0.ffn.lin1', 'distilbert.transformer.layer.0.ffn.lin2', 'distilbert.transformer.layer.1.attention.q_lin', 'distilbert.transformer.layer.1.attention.k_lin', 'distilbert.transformer.layer.1.attention.v_lin', 'distilbert.transformer.layer.1.attention.out_lin', 'distilbert.transformer.layer.1.ffn.lin1', 'distilbert.transformer.layer.1.ffn.lin2', 'distilbert.transformer.layer.2.attention.q_lin', 'distilbert.transformer.layer.2.attention.k_lin', 'distilbert.transformer.layer.2.attention.v_lin', 'distilbert.transformer.layer.2.attention.out_lin', 'distilbert.transformer.layer.2.ffn.lin1', 'distilbert.transformer.layer.2.ffn.lin2', 'distilbert.transformer.layer.3.attention.q_lin', 'distilbert.transformer.layer.3.attention.k_lin', 'distilbert.transformer.layer.3.attention.v_lin', 'distilbert.transformer.layer.3.attention.out_lin', 'distilbert.transformer.layer.3.ffn.lin1', 'distilbert.transformer.layer.3.ffn.lin2', 'distilbert.transformer.layer.4.attention.q_lin', 'distilbert.transformer.layer.4.attention.k_lin', 'distilbert.transformer.layer.4.attention.v_lin', 'distilbert.transformer.layer.4.attention.out_lin', 'distilbert.transformer.layer.4.ffn.lin1', 'distilbert.transformer.layer.4.ffn.lin2', 'distilbert.transformer.layer.5.attention.q_lin', 'distilbert.transformer.layer.5.attention.k_lin', 'distilbert.transformer.layer.5.attention.v_lin', 'distilbert.transformer.layer.5.attention.out_lin', 'distilbert.transformer.layer.5.ffn.lin1', 'distilbert.transformer.layer.5.ffn.lin2']\ntrainable params: 2,511,364 || all params: 68,874,244 || trainable%: 3.646303544181189\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240730_151535-4z2z1yfw</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/benuehlinger2/huggingface/runs/4z2z1yfw' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/benuehlinger2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/benuehlinger2/huggingface' target=\"_blank\">https://wandb.ai/benuehlinger2/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/benuehlinger2/huggingface/runs/4z2z1yfw' target=\"_blank\">https://wandb.ai/benuehlinger2/huggingface/runs/4z2z1yfw</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12630/12630 14:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.250700</td>\n      <td>0.283276</td>\n      <td>0.880734</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.267700</td>\n      <td>0.295353</td>\n      <td>0.883028</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.225600</td>\n      <td>0.289473</td>\n      <td>0.893349</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Evaluating after fine-tuning...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [55/55 00:01]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Results after fine-tuning: {'eval_loss': 0.2894730865955353, 'eval_accuracy': 0.893348623853211, 'eval_runtime': 1.3155, 'eval_samples_per_second': 662.888, 'eval_steps_per_second': 41.811, 'epoch': 3.0}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/7.69M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e99fa74517a49eaaccc24f10e664a9a"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Benuehlinger/my-peft-distilbert/commit/b4ecedd371a0cdda52445f9eacc6151cd1701f30', commit_message='Upload model', commit_description='', oid='b4ecedd371a0cdda52445f9eacc6151cd1701f30', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the base model, the full finetune model and the LoRA model on out-of-sample sentences.\n",
        "\n",
        "Note: 20 sentences of certain sentiment, 1 sentence of mixed sentiment, and 1 sentence of nonsense characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "BTKt4nJRuZpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the original, fine-tuned, and PEFT models\n",
        "original_model_name = \"distilbert-base-uncased\"\n",
        "fine_tuned_model_name = \"Benuehlinger/my-fine-tuned-distilbert\"\n",
        "peft_model_name = \"Benuehlinger/my-peft-distilbert\"  # Replace with your actual PEFT model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
        "\n",
        "# Load models and move them to the appropriate device\n",
        "original_model = AutoModelForSequenceClassification.from_pretrained(original_model_name).to(device)\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_name).to(device)\n",
        "\n",
        "# Load the PEFT model\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_name)\n",
        "peft_model = AutoModelForSequenceClassification.from_pretrained(peft_config.base_model_name_or_path).to(device)\n",
        "peft_model = PeftModel.from_pretrained(peft_model, peft_model_name).to(device)\n",
        "\n",
        "# Set up pipelines for sequence classification\n",
        "original_classifier = pipeline(\"text-classification\", model=original_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "fine_tuned_classifier = pipeline(\"text-classification\", model=fine_tuned_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "peft_classifier = pipeline(\"text-classification\", model=peft_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Generate new sentences\n",
        "new_sentences = [\n",
        "    \"I love the new movie, it's fantastic!\",\n",
        "    \"This product is terrible, I regret buying it.\",\n",
        "    \"The weather today is amazing, perfect for a picnic!\",\n",
        "    \"I feel so happy and excited about the upcoming event.\",\n",
        "    \"The service at this restaurant was extremely slow and disappointing.\",\n",
        "    \"I am very satisfied with the quality of this product.\",\n",
        "    \"The performance of the team was outstanding in the match.\",\n",
        "    \"The book I read recently was very engaging and well-written.\",\n",
        "    \"I had a great experience shopping at this store.\",\n",
        "    \"The movie I watched last night was a complete waste of time.\",\n",
        "    \"The customer service at this company needs improvement.\",\n",
        "    \"The food at the restaurant was delicious and well-prepared.\",\n",
        "    \"I strongly recommend this product to everyone.\",\n",
        "    \"The hotel I stayed in during my vacation was luxurious and comfortable.\",\n",
        "    \"The new feature added to the app is very user-friendly.\",\n",
        "    \"The concert I attended last week was amazing!\",\n",
        "    \"I had a terrible experience with the customer support.\",\n",
        "    \"The new design of the website is sleek and modern.\",\n",
        "    \"The movie had a predictable plot and was not very entertaining.\",\n",
        "    \"The delivery of my order was delayed and caused inconvenience.\",\n",
        "    \"This Movie had a good plot but weak special effects.\",\n",
        "    \"xyzdoaskeqw\"\n",
        "]\n",
        "\n",
        "# Compare predictions between the original, fine-tuned, and PEFT models for each sentence\n",
        "for sentence in new_sentences:\n",
        "    # Get predictions from all models\n",
        "    original_prediction = original_classifier(sentence)[0]\n",
        "    fine_tuned_prediction = fine_tuned_classifier(sentence)[0]\n",
        "    peft_prediction = peft_classifier(sentence)[0]\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Original Model Prediction:\", original_prediction)\n",
        "    print(\"Fine-Tuned Model Prediction:\", fine_tuned_prediction)\n",
        "    print(\"PEFT Model Prediction:\", peft_prediction)\n",
        "    print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-30T16:12:52.748309Z",
          "iopub.execute_input": "2024-07-30T16:12:52.748770Z",
          "iopub.status.idle": "2024-07-30T16:13:07.238724Z",
          "shell.execute_reply.started": "2024-07-30T16:12:52.748733Z",
          "shell.execute_reply": "2024-07-30T16:13:07.237495Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "b643490461204c67a4f5548e23511b48",
            "a288b6fc8f504b7a8008cbc5595ec008",
            "bef5d9bb7caa4960bc1c1737757f3ce0",
            "bcc792f8d0b041739b8552103b1cf3c1"
          ]
        },
        "id": "E_-HhPO6uZpT",
        "outputId": "7027f47d-4282-41c4-a8e6-e64ba7a0e0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b643490461204c67a4f5548e23511b48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a288b6fc8f504b7a8008cbc5595ec008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_config.json:   0%|          | 0.00/2.29k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bef5d9bb7caa4960bc1c1737757f3ce0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/7.69M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcc792f8d0b041739b8552103b1cf3c1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Sentence: I love the new movie, it's fantastic!\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5196182727813721}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9996926784515381}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9993448853492737}\n\nSentence: This product is terrible, I regret buying it.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5233114361763}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9982353448867798}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9869326949119568}\n\nSentence: The weather today is amazing, perfect for a picnic!\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5181533098220825}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9995694756507874}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9985925555229187}\n\nSentence: I feel so happy and excited about the upcoming event.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5294833183288574}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9995142221450806}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9964836835861206}\n\nSentence: The service at this restaurant was extremely slow and disappointing.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5242716073989868}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9987633228302002}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9968551397323608}\n\nSentence: I am very satisfied with the quality of this product.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5127538442611694}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9966243505477905}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9873968958854675}\n\nSentence: The performance of the team was outstanding in the match.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5211519598960876}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9996238946914673}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9974849224090576}\n\nSentence: The book I read recently was very engaging and well-written.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5232570171356201}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9997102618217468}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9995118379592896}\n\nSentence: I had a great experience shopping at this store.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5285704731941223}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9986575841903687}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9940318465232849}\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Sentence: The movie I watched last night was a complete waste of time.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5215651392936707}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9990159273147583}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9972447156906128}\n\nSentence: The customer service at this company needs improvement.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5050338506698608}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9972436428070068}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.6307623386383057}\n\nSentence: The food at the restaurant was delicious and well-prepared.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5133419036865234}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9996544122695923}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9976463913917542}\n\nSentence: I strongly recommend this product to everyone.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.514382541179657}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9994113445281982}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9401139616966248}\n\nSentence: The hotel I stayed in during my vacation was luxurious and comfortable.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5155490636825562}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9994975328445435}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9983770847320557}\n\nSentence: The new feature added to the app is very user-friendly.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5170662999153137}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9888491630554199}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9908081889152527}\n\nSentence: The concert I attended last week was amazing!\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5264846086502075}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9995558857917786}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.998781144618988}\n\nSentence: I had a terrible experience with the customer support.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5284839272499084}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9970923662185669}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9906895160675049}\n\nSentence: The new design of the website is sleek and modern.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5228025913238525}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.9994643330574036}\nPEFT Model Prediction: {'label': 'LABEL_1', 'score': 0.9957915544509888}\n\nSentence: The movie had a predictable plot and was not very entertaining.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5267935395240784}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.9988340735435486}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9970791339874268}\n\nSentence: The delivery of my order was delayed and caused inconvenience.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.517270565032959}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.998353123664856}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9927120208740234}\n\nSentence: This Movie had a good plot but weak special effects.\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5239971280097961}\nFine-Tuned Model Prediction: {'label': 'LABEL_0', 'score': 0.995608389377594}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.9790946841239929}\n\nSentence: xyzdoaskeqw\nOriginal Model Prediction: {'label': 'LABEL_0', 'score': 0.5439310669898987}\nFine-Tuned Model Prediction: {'label': 'LABEL_1', 'score': 0.5449567437171936}\nPEFT Model Prediction: {'label': 'LABEL_0', 'score': 0.675632655620575}\n\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}